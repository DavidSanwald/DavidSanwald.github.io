<!DOCTYPE html>
<html class="no-js">
  <head>
	<meta charset="utf-8">
	<title>Defeating the Deadly Triad&#58; | random walks and lots of &#9829;s</title>
	<meta name="description" content="">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-Frame-Options" content="sameorigin">

	<!-- CSS -->
	<link rel="stylesheet" href="/css/main.css">

	<!--Favicon-->
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

	<!-- Canonical -->
	<link rel="canonical" href="https://davidsanwald.github.io/2016/12/11/Double-DQN-interfacing-OpenAi-Gym.html">

	<!-- RSS -->
	<link rel="alternate" type="application/atom+xml" title="random walks and lots of &#9829;s" href="https://davidsanwald.github.io/feed.xml" />

	<!-- Font Awesome -->
	<link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

	<!-- Google Fonts -->
	
	<link href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic" rel="stylesheet" type="text/css">
	

	<!-- KaTeX -->
	
	<script type="text/javascript" async
	  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
	</script>
	

	<!-- Google Analytics -->
	
<!-- Begin Jekyll SEO tag v2.0.0 -->
<title>Defeating the Deadly Triad:</title>
<meta property="og:title" content="Defeating the Deadly Triad:" />
<meta name="description" content="" />
<meta property="og:description" content="" />
<link rel="canonical" href="https://davidsanwald.github.io/2016/12/11/Double-DQN-interfacing-OpenAi-Gym.html" />
<meta property="og:url" content="https://davidsanwald.github.io/2016/12/11/Double-DQN-interfacing-OpenAi-Gym.html" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2016-12-11T00:00:00+00:00" />
<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "BlogPosting",
    "headline": "Defeating the Deadly Triad:",
    "datePublished": "2016-12-11T00:00:00+00:00",
    "description": "",
    "url": "https://davidsanwald.github.io/2016/12/11/Double-DQN-interfacing-OpenAi-Gym.html"
  }
</script>
<!-- End Jekyll SEO tag -->
</head>

  <body>
    <header class="site-header">
	<div class="branding">
		
		<a href="/">
			<img class="avatar" src="/img/avatar.png" alt=""/>
		</a>
		
		<h1 class="site-title">
			<a href="/">random walks and lots of &#9829;s</a>
		</h1>
	</div>
	<nav class="site-nav">
		<ul>
			
			
			
			
			<li>
				<a class="page-link" href="/about/">
					About
				</a>
			</li>
			
			
			
			
			
			
			
			
			
			
			
			
			<!-- Social icons from Font Awesome, if enabled -->
			


<li>
	<a href="mailto:why.ever.not.berlin@gmail.com" title="Email">
		<i class="fa fa-fw fa-envelope"></i>
	</a>
</li>













<li>
	<a href="https://github.com/DavidSanwald" title="Follow on GitHub">
		<i class="fa fa-fw fa-github"></i>
	</a>
</li>





















<li>
	<a href="https://twitter.com/DavidSanwald" title="Follow on Twitter">
		<i class="fa fa-fw fa-twitter"></i>
	</a>
</li>






		</ul>
	</nav>
</header>

    <div class="content">
      <article >
  <header style="background-image: url('/')">
    <h1 class="title">Defeating the Deadly Triad&#58;</h1>
    <p class="subtitle">Implementing and training Google's  Double DQN AI </p>
    <p class="meta">
    December 11, 2016
    
    </p>
  </header>
  <section class="post-content"><p>We’re going to give birth to a deep neural learning agent. OpenAI’s Gym is awesome, it’s the first Gym where I’m sure nobody will ask me like “Do you even lift bro?” (;
So I even uploaded the agent, even though this is about sharing (which also helps me to learn) and not about competition.</p>

<p>The agent isn’t optimized for performance although it needs 51 steps, to solve the cart pole environment. If you want to, you can look at some stats and the evaluated performance here:</p>

<p><a href="https://gym.openai.com/evaluations/eval_GFtDBmuyRjCzcAkBibwYWQ">https://gym.openai.com/evaluations/eval_GFtDBmuyRjCzcAkBibwYWQ</a></p>

<iframe width="1120" height="630" src="https://www.youtube.com/embed/Ih8EfvOzBOY" frameborder="0" allowfullscreen=""></iframe>

<p>If you haven’t heard about Deepmind’s Atari playing AI, which even earned them a Nature cover <a href="#mnih2015human">(Mnih et al., 2015)</a>, I strongly recommend starting with the video above, so you know what the hype’s all about. It’s a an easy, very entertaining 3 minute watch explaining why some people get so excited about the current advancements in reinforcement learning:</p>

<p>Recently Google’s AI think tank Deepmind revolutionized reinforcement learning by introducing Deep Q-Networks (DQN). DQN is a set of techniques which made it for the first time possible to really stabilize off-policy learning methods in conjunction with function approximation (especially by deep neural networks).</p>

<p>Reinforcement learning is a lot of fun and understanding the DQN algorithm is much easier than one might think. But sometimes getting from the math in a paper to an actual implementation still can be tricky.
So I thought it could be helpful to share an easy implementation. I took some code of mine, stripped it down, removed all unnecessary clutter and used Keras to implement the networks to make it as easy as possible to understand the underlying algorithm.</p>

<p>Furthermore what Elon Musk’s no profit think tank <a href="https://openai.com/blog/">OpenAI</a> created  with their <a href="https://gym.openai.com/">Gym</a>, a framework unifying the use of learning environments for AI agents, is the most addicting playground ever. The unified interface provided by Gym makes developing, testing and comparing learning algorithms so much easier and improves reproducibility to a great extend.
So I hope I can also help by demonstrating how easy it is to interface with a Gym environment.
The example we use is the cartpole task, a classic benchmark for control. Because the state representation is low dimensional, it’s no problem at all to train an agent even on a single CPU.</p>

<p>I extended DQN to double DQN because adding Hado van Hasselt’s Double-Q-Learning <a href="#hasselt2010double">(Hasselt, 2010)</a> to DQN helps to helps to stabilize the learning process even further and isn’t hardly any more difficult to implement <a href="#van2015deep">(Van Hasselt, Guez, &amp; Silver, 2015)</a>.</p>

<p>It might helpful to know the basics of RL. If you want to get started or need a refresher I recommend the following resources:</p>

<ol>
  <li>David Silver’s amazing <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">lectures</a> on RL at UCL.</li>
  <li>Sutton and Barto’s classic book about the topic <a href="#sutton1998reinforcement">(Sutton &amp; Barto, 1998)</a> or even better the latest (free) <a href="https://webdocs.cs.ualberta.ca/~sutton/book/the-book-2nd.html">draft</a> of the upcoming 2nd edition.</li>
  <li>Recently I began working on a code centric hands-on <a href="https://davidsanwald.github.io/2016/09/12/RL-tutorial.html">notebook</a> explaining and visualizing tabular Q-learning. If you want to look into it or even give me some feedback, helping me to further improve it, this would be awesome:</li>
</ol>

<h1 id="function-approximation-and-reinforcement-learning">Function approximation and Reinforcement Learning:</h1>

<h2 id="avoiding-getting-killed-by-blue-cars-and-bowling-balls">Avoiding Getting Killed by Blue Cars and Bowling Balls</h2>

<p>For a long time most of reinforcement learning research took place on model problems with small discrete state spaces:
<img src="/img/cliff_map.png" alt="Plot of Cliffworld a gridworld developed by Stutton and Barto" />
These gridworlds are still a valuable tool for analyzing and understanding the properties of different algorithms. But one reason for the lack of success of RL outside such gridworlds was also what Richard Sutton calls the deadly triad.</p>

<p>Let’s look at the backup function for Q-learning:</p>

<script type="math/tex; mode=display">Q(s,a)\gets Q(s,a)+\alpha[r+\gamma \max_{a}Q(s',a)-Q(s,a)]</script>

<p>The Q-function maps state-action pairs to a scalar value. For a small gridworld with <script type="math/tex">4 \times 12</script> available states and 4 possible actions there are <script type="math/tex">4 \times 12 \times 4 = 512</script>  Q-value estimates needed to represent the Q-function.
 It’s easy to see, that the number of estimates will grow quickly for larger state and action spaces and that the simple tabular representation wouldn’t work at all for continuous state spaces.
 Moreover convergence to the true Q-values is only guaranteed if each state-action pair is visited a infinite number of times <a href="#watkins1992q">(Watkins &amp; Dayan, 1992)</a>, so learning the Q-function of large state-action spaces is virtually impossible.</p>

<p>It’s easy to understand that this is a problem of almost all learning. If we get hit by a red car, why will we (hopefully) jump to the side the next time a car approaches us at high velocity even if it’s a blue car this time? And what about deciding whether to catch a spherical object or not? If we successfully caught a basketball, why won’t most people try the same thing with a bowling ball?</p>

<p>Because we never experience the exact same situation twice, we have to <em>generalize</em> from multiple past experiences to  successfully evaluate new situations.</p>

<h2 id="running-into-the-deadly-triad">Running Into the Deadly Triad</h2>

<p>The proven ability (imposing only some mild conditions) of neural networks to approximate arbitrary continuous functions  by superposition the activation functions of its neurons <a href="#csaji2001approximation">(Csáji, 2001)</a> makes it only a natural choice to use them to represent the Q-function.
So the approximate Q-function</p>

<script type="math/tex; mode=display">Q(s,a) \approx Q(s,a, \mathbf{w})</script>

<p>is now parameterized by the weights <script type="math/tex">\mathbf{w}</script> of the neural network.
But this naive approach will be highly unstable. Why’s that so? We’re encountering what Sutton calls the <em>deadly triad</em>:</p>

<p>Learning combining the following three conditions is always most likely to diverge:</p>

<ol>
  <li>off-policy learning, like it’s the case with Q-learning due to the $max$ operator in the Bellman update</li>
  <li>scalable function approximation</li>
  <li>bootstrapping, meaning estimating estimates from estimates, which is the main principle of <em>Temporal-Difference</em> learning methods</li>
</ol>

<p>Even linear function approximation in seemingly very simple environments can lead to instable, diverging Q-functions. I strongly recommend taking a look at <em>Baird’s counterexample</em> <a href="#baird1995residual">(Baird &amp; others, 1995)</a>. Leemond Baird constructed an example using the simplest MDP one could think of, where linear function approximation still needs to divergence of the Q-function.</p>

<h2 id="dqn-to-the-rescue">DQN to the Rescue</h2>

<p>Until the development of DQN it has impossible to stabilize learning under the conditions of the <em>deadly triad</em>.
DQN provides a set of three methods to stabilize the learning process. The first one which is clipping the rewards to a fixed range, is pretty straight forward.</p>

<ol>
  <li>Clipping the TD-error to a fixed interval of <script type="math/tex">[-1, 1]</script></li>
  <li>Experience replay</li>
  <li>Replacing one single network by the use of two separate networks, one online network and one target network</li>
</ol>

<p>The first one is pretty straight forward. So let’s talk about experience replay and the target network.</p>

<h3 id="experience-replay-or-flashbacks">Experience Replay or Flashbacks</h3>
<p>Neural networks have been successfully used for <em>supervised learning</em> for years. So the main idea behind DQN is to find a way of brining training of the network in RL closer to the supervised setting.
Experiences consisting of the tuple  <script type="math/tex">(s, a, s', r)</script>, state, action, nextstate, reward, are not directly used to update the Q-function. Instead they are stored in a replay memory of fixed size. The Q-function now is updated by sampling a random batch of memory tuples at each timestep from this memory. This has several advantages:</p>

<ol>
  <li>Now it is possible to use Mini-batch gradient descent to optimize the loss of the network, which reduces variance.</li>
  <li>At each timestep it is now possible to learn from multiple samples not just from the recent transition.</li>
  <li>Learning implies the assumption of the samples being <em>i.i.d</em>. Because the samples are generated by a sequence of subsequent steps by the agent, they violate this assumption. Decoupling generating samples from learning samples by using the memory as a proxy brings the distribution of samples used for learning closer to the  <em>i.i.d</em> assumption.</li>
</ol>

<p>Couldn’t resist to call this <strong>Flashback</strong>, it sounds so much cooler. Sorry (:</p>

<h3 id="using-a-fixed-target-network">Using a fixed target network</h3>

<p>Let’s look at the backup for the Q-function again:</p>

<script type="math/tex; mode=display">Q(s,a)\gets Q(s,a)+\alpha[r+\gamma \max_{a}Q(s',a)-Q(s,a)]</script>

<p>Using a neural network, both the targets given by <script type="math/tex">\gamma \max_{a}Q(s',a, \mathbf{w})</script> as well as the updated values <script type="math/tex">Q(s,a,\mathbf{w})</script> depend on the weights <script type="math/tex">\mathbf{w}</script> of the neural network.
So a weight update that increases
<script type="math/tex">Q(s,a,\mathbf{w})</script>
potentially also increases
<script type="math/tex">\gamma \max_{a}Q(s',a, \mathbf{w})</script>
which could result in feedback.
To address this issue two separate networks are used. The online network parametrized by <script type="math/tex">\mathbf{w^⁻}</script> is used to generate the Q-values for epsilon greedy action selection.
To compute the targets
<script type="math/tex">Y_{DQN}=\gamma \max_{a}Q(s',a, \mathbf{w^⁻})</script>
a second network, the target network parametrized by <script type="math/tex">\mathbf{w^⁻}</script> is used. At each timestep the weights of the online network are updated by optimizing the MSE loss function</p>

<script type="math/tex; mode=display">\mathcal{L}(w) = \mathbb{E}_{s,a,r,s'\sim \mathcal{D}}[(r+\gamma\max_{a}Q(s',a,\mathbf{w^{-}})-Q(s,a,\mathbf{w}))^{2}]</script>

<p>via mini-batch gradient descent.
The weights <script type="math/tex">\mathbf{w^⁻}</script> of the target network are kept fixed and only periodically updated by copying the weights from the online network:
<script type="math/tex">\mathbf{w^⁻}\gets \mathbf{w}</script>
This avoids destabilizing feedback loops in the update of the Q-function. Only after the weight updates DQN generated for a short time to  the simple Q-learning case because
<script type="math/tex">\mathbf{w^⁻}=\mathbf{w}</script>, it’s practically the same as using a single network.</p>

<h3 id="tackling-value-overestimation-by-extending-dqn-to-double-dqn">Tackling Value Overestimation by Extending DQN to Double DQN</h3>

<p>To be honest, the cart-pole example we’re using here, won’t benefit much from extending DQN to Double DQN. This is due to the fact that overestimation bias increases with the number of possible actions <a href="#van2015deep">(Van Hasselt, Guez, &amp; Silver, 2015)</a> and the classic cart-pole domain is a bang bang control problem (just discrete actions, “on or off”) with only 2 available actions.
Nevertheless the code will way more stable in other environments. Because RL is general and not problem specific, you can use the same code for a wide variety of problems and when a convolution layer is added and the parameters are tuned it performs well on raw pixels. The benefits of Double DQN will also increase, when confronted with noisy environments and especially noisy reward signals.
So adding double DQN right away is worth it.
While the recent paper  <a href="#van2015deep">(Van Hasselt, Guez, &amp; Silver, 2015)</a> about double DQN is accessible if one invests a little time, the original paper  by Hado van Hasselt  introducing Double-Q-Learning is a really challenging read <a href="#hasselt2010double">(Hasselt, 2010)</a>.
I try to provide some intuition here but if you really want to get into the mathematics behind the overestimation bias I recommend the following steps:</p>

<ol>
  <li>Begin with Jensen’s Inequality. <a href="https://youtu.be/HfCb1K4Nr8M">This</a> video is a great starting point before looking at the proofs.</li>
  <li>Read the passage on Double-Q-Learning in the draft version of Sutton and Barto’s book.</li>
  <li>Read the Double DQN paper <a href="#van2015deep">(Van Hasselt, Guez, &amp; Silver, 2015)</a></li>
  <li>Finally read Thrun and Schwartz’ paper about function approximation and overestimation biases in RL <a href="#thrun1993issues">(Thrun &amp; Schwartz, 1993)</a> and Hado van Hasselt’s original paper introducing Double Q-learning.</li>
</ol>

<p>Basically Q-learning uses <script type="math/tex">\mathbb{E}[\max_{a}Q(s',a)]</script> to approximate  <script type="math/tex">\max_{a}\mathbb{E}[Q(s',a)]</script>, which is always less or equal.
Using the max over all Q-values, we always select the one, with the highest probability of overestimation (when there’s noise or function approximation). It’s easy to understand that expectation of these estimates now has an upwards bias.</p>

<p>Double DQN decomposes the max operation for the target values into action selection and action evaluation. The targets are now</p>

<script type="math/tex; mode=display">Y_{DDQN}=r+\gamma Q(s', \mathop{\arg\,\max}\limits_a Q(s',a,\mathbf{w}), \mathbf{w^⁻})</script>

<p>so the argmax of the Q-values of the online network for a particular state maps this state to an action, now the target network’s Q-value is a function of this state and the action selected by the online network.</p>

<h1 id="shut-up-and-show-me-the-code">Shut up and show me the code!</h1>

<p>I sacrificed some DRYness and extendability of the code, to make the underlying algorithm as understandable as possible. To keep it simple I also sinned and used global variables for configuration to avoid bloated commandline parsing.
If something isn’t clear, feel free to ask my anytime. Here I want to explain the parts I consider most important.</p>

<p>The whole code is consists of the following parts:</p>

<ul>
  <li><code class="highlighter-rouge">Experiment</code>: Wrapper around the Gym environment. Also containing the control flow for conducting the whole training as well as the agent-environment loop.</li>
  <li><code class="highlighter-rouge">Agent</code>: Learning agent, containing the learning logic as well as an instance of <code class="highlighter-rouge">NN</code> and the <code class="highlighter-rouge">ReplayMemory</code></li>
  <li><code class="highlighter-rouge">NN</code>: Wrapping the online network <code class="highlighter-rouge">model</code> as well as the target network <code class="highlighter-rouge">model_t</code></li>
  <li><code class="highlighter-rouge">ReplayMemory</code>: Storing the experience tuples and providing randomly sampled experience batches</li>
  <li><code class="highlighter-rouge">EpsilonUpdater</code>: Leftover from my own code. Using the observer pattern, to control greediness of action selection. Kept it, because it makes it simple to extend for adaptive control of other parameters like the learning rate or target network update frequency.</li>
  <li><code class="highlighter-rouge">Utils</code> Just some functions for reshaping numpy arrays</li>
</ul>

<p>I wrapped the Gym environment in an experiment object which holds the agent-environment loop and all runtime logic for training.</p>

<h3 id="experiment">Experiment</h3>

<p>The main part is the <code class="highlighter-rouge">run_episode</code> method, which also contains the agent-environment loop.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">run_episode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s_</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s_</span></code></pre></figure>

<p>The interface of a Gym environment is very simple and consists of just 3 methods:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="n">reset</span><span class="p">()</span> <span class="c"># resets environment and returns the start state</span>
 <span class="n">step</span><span class="p">()</span> <span class="c"># takes action, returns newstate, reward and true of false for being terminal</span>
 <span class="n">render</span> <span class="p">()</span> <span class="c"># renders environment</span>

 </code></pre></figure>

<h3 id="agent">Agent</h3>

<p>Exposes just two methods to interface with the experiment. I tried to resemble the classic agent-environment interface as close as possible.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">act</span><span class="p">()</span> <span class="c"># takes state, returns action</span>
<span class="n">learn</span><span class="p">()</span> <span class="c"># takes state, action, newstate, reward, done bool and computes back ops</span></code></pre></figure>

<p>The <code class="highlighter-rouge">_make_batch()</code> method queries the <code class="highlighter-rouge">ReplayMemory()</code> for a batch of random samples.
It also computes the targets, so it holds all the relevant ingredients for DQN and double DQN.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">run_episode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">s_</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">learn</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">s_</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s_</span></code></pre></figure>

<h3 id="nn">NN</h3>

<p>The brain of the agent. I used <script type="math/tex">\tanh</script> as activation function because I think that rectified linear units (ReLU) don’t add
any value for such small networks and low dimensional inputs. Furthermore we need some sort of gradient clipping, if we use ReLUs,
because even smaller learning rates can lead to dying ReLUs.
If you want to learn from pixels, you have to change just a few things:</p>

<ol>
  <li>Add 2 or 3 convolution layers</li>
  <li>Increase general network size and add regularization (start with dropout, if needed try L2-norm weight regularization next)</li>
  <li>Tweak the learning rate, now you also could benefit from switching to ReLU activation functions</li>
  <li>Add some gradient clipping, don’t be so hard on the poor ReLUs &lt;3</li>
</ol>

<p>Optimization wise we use simple stochastic gradient descent. I think it’s a good baseline.
The input doesn’t contain any spacial structure, where we could profit from convolution, so we just use simple fully connected layer.
In my experience really small and and shallow networks perform best on such small problems. For higher dimensional inputs, especially for learning from raw pixel input or something like that, I recommend to be quite generous with layer size and use regularization to deal with overfitting.</p>

<p>I left this check if the loss is <code class="highlighter-rouge">NaN</code> in the code. If using ReLUs you should notice if the die and you have to start over (:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">flashback</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_batch</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">NN</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">])</span><span class="o">.</span><span class="nb">any</span><span class="p">():</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Warning, loss is {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">))</span>
    <span class="k">pass</span></code></pre></figure>

<p>Of course this is the very least. I didn’t want to clutter the code with all the bookkeeping stuff but if you don’t want to be blind, you wanna also compute the max Q-values, the loss and the values of the weights over time. You can’t have too many plots. Ever! (:</p>

<h3 id="replaymemory">ReplayMemory</h3>

<p>I used a double-ended queue to implement the maximum memory capacity. If the memory is queried for a batch and the batch size exceeds the current number of stored samples in the memory the memory returns the maximum number of samples available.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">ReplayMemory</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">capacity</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">deque</span><span class="p">([],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">capacity</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">store</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exp</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">exp</span><span class="p">)</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">samples</span><span class="p">))</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">samples</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">samples</span></code></pre></figure>

<p>This works nice but if we want to implement prioritized replay, we need to use a Tree structure, so we’re able to achieve <script type="math/tex">\mathcal{O}(n\log{}n)</script> search complexity (best case).
But for now we’re happy with our simple deque (:</p>

<h3 id="epsilonupdater">EpsilonUpdater</h3>

<p>The <code class="highlighter-rouge">EpsilonUpdater</code> uses a push-oriented observer pattern. It’s easy to add more observers, to control additional parameters like the learning rate or update frequency at runtime. You can also add more events.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">EpsilonUpdater</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span> <span class="o">=</span> <span class="n">agent</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">event</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">event</span> <span class="o">==</span> <span class="s">'step_done'</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_update</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">switch_learning</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">epsilon_update</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">epsilon_min</span> <span class="o">+</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">epsilon_max</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">epsilon_min</span><span class="p">)</span> <span class="o">*</span> <span class="n">exp</span><span class="p">(</span>
                <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">epsilon_decay</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">step_count_total</span><span class="p">))</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">switch_learning</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">step_count_total</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">learning_start</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">learning_switch</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">pass</span></code></pre></figure>

<p>The observer are attached via the agent’s <code class="highlighter-rouge">add_observer()</code> method:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">add_observer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">observer</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">observers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">observer</span><span class="p">)</span>
    <span class="k">pass</span></code></pre></figure>

<p>The agent can push events to the observers by calling them using their magic
<code class="highlighter-rouge">__call__</code> method, which takes a simple string as identification for different events.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">notify</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">event</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">observer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">observers</span><span class="p">:</span>
        <span class="n">observer</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
    <span class="k">pass</span></code></pre></figure>

<h2 id="poledancing">Poledancing</h2>

<p>Here’s some video of our beauty:</p>

<iframe width="1120" height="630" src="https://www.youtube.com/embed/I7oPYEeSVfc" frameborder="0" allowfullscreen=""></iframe>

<p>Some performance stats can be found here:</p>

<p><a href="https://gym.openai.com/evaluations/eval_GFtDBmuyRjCzcAkBibwYWQ">https://gym.openai.com/evaluations/eval_GFtDBmuyRjCzcAkBibwYWQ</a></p>

<p>The code can be found here:</p>

<p><a href="https://github.com/DavidSanwald/DDQN">https://github.com/DavidSanwald/DDQN</a></p>

<h2 id="last-words">Last words</h2>

<p>I’m happy if this helps someone or I could even share some of the joy I take in such things.
If I can help you in anyway or of something isn’t clear, just contact me.
Also drop me a note if you want to start some project or just want to talk.</p>

<ol class="bibliography"><li><span id="mnih2015human">Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … others. (2015). Human-level control through deep reinforcement learning. <i>Nature</i>, <i>518</i>(7540), 529–533.</span></li>
<li><span id="hasselt2010double">Hasselt, H. V. (2010). Double Q-learning. In <i>Advances in Neural Information Processing Systems</i> (pp. 2613–2621).</span></li>
<li><span id="van2015deep">Van Hasselt, H., Guez, A., &amp; Silver, D. (2015). Deep reinforcement learning with double Q-learning. <i>CoRR, Abs/1509.06461</i>.</span></li>
<li><span id="sutton1998reinforcement">Sutton, R. S., &amp; Barto, A. G. (1998). <i>Reinforcement learning: An introduction</i> (Vol. 1). MIT press Cambridge.</span></li>
<li><span id="watkins1992q">Watkins, C. J. C. H., &amp; Dayan, P. (1992). Q-learning. <i>Machine Learning</i>, <i>8</i>(3-4), 279–292.</span></li>
<li><span id="csaji2001approximation">Csáji, B. C. (2001). Approximation with artificial neural networks. <i>Faculty of Sciences, Etvs Lornd University, Hungary</i>, <i>24</i>, 48.</span></li>
<li><span id="baird1995residual">Baird, L., &amp; others. (1995). Residual algorithms: Reinforcement learning with function approximation. In <i>Proceedings of the twelfth international conference on machine learning</i> (pp. 30–37).</span></li>
<li><span id="thrun1993issues">Thrun, S., &amp; Schwartz, A. (1993). Issues in using function approximation for reinforcement learning. In <i>Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum</i>.</span></li></ol>
</section>
</article>

<!-- Post navigation -->


<!-- Disqus -->

<div class="comments">
  <div id="disqus_thread"></div>
<script type="text/javascript">
	var disqus_shortname = 'random-walks';
	(function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>


<!-- Muut -->


    </div>
    
<script src="/js/katex_init.js"></script>



<footer class="site-footer">
	<p class="text">&#9829; &#9829; &#9829;  to  <a href="https://rohanchandra.github.io/project/type/">Type Theme</a>
</p>
</footer>


  </body>
</html>
